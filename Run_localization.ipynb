{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image, PIL.ImageFont, PIL.ImageDraw\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow_datasets as tfds\n",
    "import tensorflow.keras as ks\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree as xt\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "from glob import glob\n",
    "from tensorflow.keras.models import Model\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as ks\n",
    "import numpy as np\n",
    "\n",
    "# Define your model architecture\n",
    "\n",
    "def feature_extractor(inputs):\n",
    "    x = ks.layers.Conv2D(16, activation='relu', kernel_size=3, input_shape=(227, 227, 3))(inputs)\n",
    "    x = ks.layers.AveragePooling2D((2, 2))(x)\n",
    "    x = ks.layers.Conv2D(32, activation='relu', kernel_size=3)(x)\n",
    "    x = ks.layers.AveragePooling2D((2, 2))(x)\n",
    "    x = ks.layers.Conv2D(64, activation='relu', kernel_size=3)(x)\n",
    "    x = ks.layers.AveragePooling2D((2, 2))(x)\n",
    "    return x\n",
    "\n",
    "def dense_layer(inputs):\n",
    "    x = ks.layers.Flatten()(inputs)\n",
    "    x = ks.layers.Dense(128, activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "def classifier(inputs):\n",
    "    classifier_output = ks.layers.Dense(5, activation='softmax', name='classifier')(inputs)\n",
    "    return classifier_output\n",
    "\n",
    "def bounding_box_regression(inputs):\n",
    "    bounding_box_regression_output = ks.layers.Dense(4, name='bounding_box')(inputs)\n",
    "    return bounding_box_regression_output\n",
    "\n",
    "def final_model(inputs):\n",
    "    feature_cnn = feature_extractor(inputs)\n",
    "    dense_output = dense_layer(feature_cnn)\n",
    "    classification_output = classifier(dense_output)\n",
    "    bounding_box = bounding_box_regression(dense_output)\n",
    "    \n",
    "    model = ks.Model(inputs=inputs, outputs=[classification_output, bounding_box])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create an instance of the model\n",
    "inputs = ks.Input(shape=(227, 227, 3))\n",
    "model = final_model(inputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss={'classifier': 'categorical_crossentropy', 'bounding_box': 'mean_squared_error'}, metrics={'classifier': 'accuracy', 'bounding_box': 'mean_squared_error'})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# prepare your training data as .xml \n",
    "\n",
    "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<annotation>\n",
    "<folder>images</folder>\n",
    "<filename>16922e93-734ba0ad-orange_downright.jpg</filename>\n",
    "<source>\n",
    "<database>MyDatabase</database>\n",
    "<annotation>COCO2017</annotation>\n",
    "<image>flickr</image>\n",
    "<flickrid>NULL</flickrid>\n",
    "<annotator>1</annotator>\n",
    "</source>\n",
    "<owner>\n",
    "<flickrid>NULL</flickrid>\n",
    "<name>Label Studio</name>\n",
    "</owner>\n",
    "<size>\n",
    "<width>227</width>\n",
    "<height>227</height>\n",
    "<depth>3</depth>\n",
    "</size>\n",
    "<segmented>0</segmented>\n",
    "<object>\n",
    "<name>orange</name>\n",
    "<pose>Unspecified</pose>\n",
    "<truncated>0</truncated>\n",
    "<difficult>0</difficult>\n",
    "<bndbox>\n",
    "<xmin>129</xmin>\n",
    "<ymin>115</ymin>\n",
    "<xmax>226</xmax>\n",
    "<ymax>226</ymax>\n",
    "</bndbox>\n",
    "</object>\n",
    "</annotation>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data and prepare for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = sorted(glob('training_images/*.xml'))# get all elements in path with xml extension\n",
    "y = [] #create a list to store our targets\n",
    "for file in path:# loop over each file path and parse it\n",
    "    label = 0\n",
    "    info = xt.parse(file) \n",
    "    root = info.getroot() \n",
    "    obj = root.find('object')\n",
    "    \n",
    "    label_obj = (obj.find('name').text)\n",
    "\n",
    "    if \"cucumber\" in label_obj:  \n",
    "        label = 1\n",
    "    elif \"eggplant\" in label_obj:\n",
    "        label = 2\n",
    "    elif \"mushroom\" in label_obj:\n",
    "        label = 3\n",
    "    elif \"orange\" in label_obj:\n",
    "        label = 4\n",
    "\n",
    "\n",
    "    bndbox = obj.find('bndbox')\n",
    "    xmin = (bndbox.find('xmin').text)\n",
    "    xmax = (bndbox.find('xmax').text)\n",
    "    ymin = (bndbox.find('ymin').text)\n",
    "    ymax = (bndbox.find('ymax').text)\n",
    "    \n",
    "    #append the results to our target\n",
    "    y.append([int(label),int(xmin),int(ymin),int(xmax),int(ymax)])\n",
    "\n",
    "\n",
    "\n",
    "# images \n",
    "\n",
    "\n",
    "X = []\n",
    "images_path = sorted(glob('training_images/*.jpg'))# get all elements in path with jpg extension\n",
    "\n",
    "for image in images_path:#loop over image path\n",
    "    img = cv2.imread(image, cv2.COLOR_BGR2RGB)# read the image in rgb mode\n",
    "    X.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check bbox\n",
    "\n",
    "img_example = X[1]\n",
    "bbox = y[1][1:]\n",
    "y[1][1:] #view the bbox coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test\n",
    "X = np.array(X)\n",
    "y =  np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1)\n",
    "\n",
    "\n",
    "categ_train = y_train[:,0:1]\n",
    "categ_test = y_test[:,0:1]\n",
    "bbox_train = y_train[:,1:]\n",
    "bbox_test = y_test[:,1:]\n",
    "categ_train.shape,categ_test.shape,bbox_train.shape,bbox_test.shape\n",
    "\n",
    "categ_train = tf.keras.utils.to_categorical(categ_train)\n",
    "categ_test = tf.keras.utils.to_categorical(categ_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "steps_per_epoch = 60000/(batch_size*5)\n",
    "\n",
    "history = model.fit(X_train,[categ_train,bbox_train], steps_per_epoch=steps_per_epoch, epochs=100, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r= model.fit(X_train,[categ_train,bbox_train],epochs=100,validation_data=(X_test,[categ_test,bbox_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(r.history)\n",
    "results.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(results,y=[results['classifier_loss'],results['val_classifier_loss']],template=\"seaborn\",color_discrete_sequence=['#05445E','#75E6DA'])\n",
    "fig.update_layout(   \n",
    "    title_font_color=\"#75E6DA\", \n",
    "    xaxis=dict(color=\"#75E6DA\",title='Epochs'), \n",
    "    yaxis=dict(color=\"#75E6DA\")\n",
    " )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path, model, from_url=False):\n",
    "    \"\"\"\n",
    "    passes an image to a trained model and returns annotated image\n",
    "\n",
    "    Args:\n",
    "    path:  path containing the image or a list of paths\n",
    "    model:  pretrained model instance\n",
    "    from_url: (boolen) to check if the path is a url\n",
    "    \"\"\"\n",
    "    # Check if the path is a list\n",
    "    if type(path) == list:\n",
    "        # loop over each path and read the image\n",
    "        for p in path:\n",
    "            if from_url:\n",
    "                image = io.imread(p)\n",
    "            else:\n",
    "                image = cv2.imread(p)\n",
    "    # Check if the type of the path is a string\n",
    "    elif type(path) == str:\n",
    "            # read the given image\n",
    "            if from_url:\n",
    "                image = io.imread(path)\n",
    "            else:\n",
    "                image = cv2.imread(path)\n",
    "    # Resize the image into the appropriate shape    \n",
    "    image = cv2.resize(image,(227,227))\n",
    "    # get model predictions\n",
    "    categ,bbox = model.predict(np.array([image]))\n",
    "    # Get class of the highest given probability\n",
    "    categ = np.argmax(categ)\n",
    "    # flatten the bounding box array and cast it into integer\n",
    "    bbox = bbox.flatten()\n",
    "    bbox = bbox.astype(int)\n",
    "    # draw a rectangle on the image using the predicted bbox coordinates\n",
    "    image = cv2.rectangle(image,(bbox[0],bbox[1]),(bbox[2],bbox[3]),(255,0,0),1)\n",
    "    # set prediction class to an empty string\n",
    "    prediction = ''\n",
    "    # set the prediction to the name of the given class according to the model's prediction\n",
    "    if categ == 1:\n",
    "        prediction = 'Cucumber'\n",
    "    elif categ == 2:\n",
    "        prediction = 'Egg Plant'\n",
    "    elif categ == 3:\n",
    "        prediction = 'Mushroom'\n",
    "    elif categ == 4:\n",
    "        prediction = 'orange'\n",
    "    # Put the text of the prediction on the image\n",
    "    final_img = cv2.putText(image,prediction,(bbox[0],bbox[1]-4),cv2.FONT_HERSHEY_SIMPLEX,0.3,(255,0,0),1,255)\n",
    "    # Plot the image\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(final_img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = 'my_data/images'\n",
    "# Create the list of image file paths\n",
    "image_paths = [os.path.join(image_directory, filename) for filename in os.listdir(image_directory) if filename.endswith('.jpg')]\n",
    "\n",
    "for i in range(1,10):\n",
    "    predict(image_paths[i],model,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('image_path',model,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
